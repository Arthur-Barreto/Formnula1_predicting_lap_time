{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==== Banco de Variaveis e Resultados ====\n",
    "#Tipos de Score\n",
    "maior_score_test = 0\n",
    "melhor_id_loja   = 0\n",
    "pior_score_test  = 1 # pq esse é maior dado, dai sera menor que todos os dados analizamos\n",
    "pior_id_loja     = 0\n",
    "lojas_alpha_maior_90 = []\n",
    "explained_variance_m1 = []\n",
    "#Informações Unitarias (Cada Store ID)\n",
    "Coef_Reg = []\n",
    "Mean_Squared = []\n",
    "R_Squared = []\n",
    "mean_max_abs_error_m1 = []\n",
    "#Predição\n",
    "Y_Prediction = []\n",
    "Y_Train = []\n",
    "Importance_m3 = []\n",
    "\n",
    "# vetor para guardar as informações do cross validation e plotagem de gráficos futura\n",
    "# vetores com final m guardam a média dos valores\n",
    "r2_cross_m = []\n",
    "mean_square_cross_m = []\n",
    "explained_var_cross_m = []\n",
    "mean_max_error_m = []\n",
    "# já os vetores com std no final, guardam o desvio padrão da amostra\n",
    "r2_cross_std = []\n",
    "mean_square_cross_std = []\n",
    "explained_var_cross_std = []\n",
    "mean_max_error_std = []\n",
    "\n",
    "for loja in range(0,45): \n",
    "    #==== Criando Variaveis e Objetos ====\n",
    "    regr = linear_model.LinearRegression()\n",
    "    \n",
    "    #=== Treinamento do Modelo ====\n",
    "    # Treinamento Modelo - Usando X_Train e Y_Train\n",
    "    regr.fit(Xi[loja][0], Yi[loja][0])\n",
    "    # Predição de Y - Usando X_Test\n",
    "    sales_y_pred = regr.predict(Xi[loja][1])\n",
    "    Y_Prediction.append(sales_y_pred)\n",
    "\n",
    "    #Create linear regression object\n",
    "    score_train = regr.score(Xi[loja][0], Yi[loja][0])\n",
    "    score_test = regr.score(Xi[loja][1], Yi[loja][1])\n",
    "    \n",
    "    #==== Coletando Informações Sobre Modelo ====\n",
    "    #Coletando diferentes tipos de Score:\n",
    "    if score_test > maior_score_test:\n",
    "        maior_score_test = score_test\n",
    "        melhor_id_loja = loja + 1 #Adição é realizada por materia de Index e Apresentação\n",
    "    if score_test < pior_score_test:\n",
    "        pior_score_test = score_test\n",
    "        pior_id_loja = loja + 1 #Adição é realizada por materia de Index e Apresentação\n",
    "    if score_test >= 0.9:\n",
    "        lojas_alpha_maior_90.append(loja+1)\n",
    "    #Coeficiente de regressão\n",
    "    const = [regr.intercept_]\n",
    "    const.extend(regr.coef_) #.extend = Extender lista para adicionar const ao inicio\n",
    "    Coef_Reg.append(const)\n",
    "    \n",
    "    #Média Quadratica\n",
    "    m = mean_squared_error(Yi[loja][1], sales_y_pred)\n",
    "    Mean_Squared.append(m)\n",
    "    #R Quadrado\n",
    "    r = r2_score(Yi[loja][1], sales_y_pred)\n",
    "    R_Squared.append(r)\n",
    "    # score da variancia, usando o modulo explained_variance_score\n",
    "    explained_variance_m1.append(explained_variance_score(Yi[loja][1],sales_y_pred)) \n",
    "    # mean_absolute_error\n",
    "    mean_max_abs_error_m1.append(mean_absolute_error(Yi[loja][1],sales_y_pred))\n",
    "    \n",
    "    # preparando as features para a validação cruzada\n",
    "    # precisamos separar as colunas da loja especificada, para a biblioteca trabalhar em cima da loja e feature correta\n",
    "    # Como Xteste e Xtreinamento já é a feature que eu preciso, basta concatenar a passar esse valar para a crossvalid\n",
    "    x = pd.concat([Xi[loja][0],Xi[loja][1]],ignore_index = True)\n",
    "    y = pd.concat([Yi[loja][0],Yi[loja][1]],ignore_index = True)\n",
    "    \n",
    "    # na validação cruzada o argumento cv é o número de partições que faremos\n",
    "    # como temos 52 dados, e 52 = 2²*13, vamos dividir em 13 grupos de 4 amostras\n",
    "    \n",
    "    # metricas para a validaçao cruzada\n",
    "    #'r2','neg_mean_squared_error','explained_variance'\n",
    "    # as metricas possuem um mnemonico correto, segue o link onde encontrar o mnemonico correto\n",
    "    # https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "    # para r²\n",
    "    r2 = cross_val_score(regr,x,y, scoring = 'r2', cv = 4)\n",
    "    r2_cross_m.append(r2.mean())\n",
    "    r2_cross_std.append(r2.std())\n",
    "    # erro médio quadratico\n",
    "    media = cross_val_score(regr,x,y, scoring = 'neg_mean_squared_error', cv = 4)\n",
    "    mean_square_cross_m.append(media.mean())\n",
    "    mean_square_cross_std.append(media.std())\n",
    "    # explanaid variance\n",
    "    var_exp = cross_val_score(regr,x,y, scoring = 'explained_variance', cv = 4)\n",
    "    explained_var_cross_m.append(var_exp.mean())\n",
    "    explained_var_cross_std.append(var_exp.std())\n",
    "    # mean_absolute_error\n",
    "    mean_max_error = cross_val_score(regr,x,y, scoring = 'neg_mean_absolute_error', cv = 4)\n",
    "    mean_max_error_m.append(mean_max_error.mean())\n",
    "    mean_max_error_std.append(mean_max_error.std())\n",
    "    \n",
    "print(f'A loja com o maior score de test foi {melhor_id_loja}, cujo score foi de {maior_score_test}')\n",
    "print(f'A loja com o pior score de test foi {pior_id_loja}, cujo score foi de {pior_score_test}')\n",
    "print(f'As lojas cujo score foi maior ou igual a 90% foram: {lojas_alpha_maior_90}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==== Banco de Variaveis e Resultados ====\n",
    "#Informações unitarias (Cada Store ID)\n",
    "Coef_Reg2 = []\n",
    "Mean_Squared2 = []\n",
    "R_Squared2 = []\n",
    "explained_variance_m2 = []\n",
    "mean_max_abs_error_m2 = []\n",
    "#Predição\n",
    "Y_Prediction2 = []\n",
    "Importance_m2 = []\n",
    "#Definindo Figura do gráfico 3D\n",
    "fig = plt.figure(figsize=(25,16))\n",
    "#Definindo 2 Eixos com Projeção 3d\n",
    "ax = fig.add_subplot(121,projection=\"3d\")\n",
    "ax2 = fig.add_subplot(122,projection=\"3d\")\n",
    "\n",
    "#Setando Labels e Título\n",
    "ax.set_xlabel(\"Avr Temperature\", fontsize=15)\n",
    "ax.set_ylabel(\"Avr Fuel Price\", fontsize=15)\n",
    "ax.set_zlabel(\"Weekly Sales\",fontsize=15)\n",
    "ax2.set_xlabel(\"Avr CPI\",fontsize=15)\n",
    "ax2.set_ylabel(\"Avr Unemployment\",fontsize=15)\n",
    "ax2.set_zlabel(\"Weekly Sales\",fontsize=15)\n",
    "ax2.set_title(\"Avr CPI x Avr Unemp x Weekly Sales\",fontsize=20, fontweight=750)\n",
    "ax.set_title(\"Avr Temp x Avr Fuel x Weekly Sales\",fontsize=20, fontweight=750)\n",
    "#Definindo angulatura de visão\n",
    "ax.view_init(35,35)\n",
    "ax2.view_init(35,35)\n",
    "\n",
    "# vetor para guardar as informações do cross validation e plotagem de gráficos futura\n",
    "# vetores com final m guardam a média dos valores\n",
    "r2_cross_m_m2 = []\n",
    "mean_square_cross_m_m2 = []\n",
    "explained_var_cross_m_m2 = []\n",
    "mean_max_error_m_m2 = []\n",
    "\n",
    "# já os vetores com std no final, guardam o desvio padrão da amostra\n",
    "r2_cross_std_m2 = []\n",
    "mean_square_cross_std_m2 = []\n",
    "explained_var_cross_std_m2 = []\n",
    "mean_max_error_std_m2 = []\n",
    "\n",
    "for loja in range(0,45):\n",
    "    #==== Criando Variaveis e Objetos ====\n",
    "    # Regrassão 2 - TreeRegressor\n",
    "    regr_1 = DecisionTreeRegressor(max_depth=2)\n",
    "    regr_1.fit(Xi[loja][0], Yi[loja][0]) #Dados de Treinamento\n",
    "    # Previsão\n",
    "    sales_y_pred2 = regr_1.predict(Xi[loja][1])\n",
    "    Y_Prediction2.append(sales_y_pred2)\n",
    "    \n",
    "    #Coeficiente de regressão\n",
    "    const2 = [regr.intercept_]\n",
    "    const2.extend(regr.coef_)\n",
    "    Coef_Reg2.append(const)\n",
    "    \n",
    "    #Média Quadratica\n",
    "    m2 = mean_squared_error(Yi[loja][1], sales_y_pred2)\n",
    "    Mean_Squared2.append(m2)\n",
    "    #R Quadrado\n",
    "    r2 = r2_score(Yi[loja][1], sales_y_pred2)\n",
    "    R_Squared2.append(r2)\n",
    "    # explanaid variance\n",
    "    explained_variance_m2.append(explained_variance_score(Yi[loja][1],sales_y_pred2))\n",
    "    # Mean_absolute_error\n",
    "    mean_max_abs_error_m2.append(mean_absolute_error(Yi[loja][1],sales_y_pred2))\n",
    "    \n",
    "    #Plot Gráfico 3D - Avr's Temp, Fuel, CPI, Unemp x Weekly_Sales\n",
    "    ax.scatter3D(Xi[loja][1][\"Avr Temp\"],Xi[loja][1][\"Avr Fuel\"],sales_y_pred2,s=80) \n",
    "    ax2.scatter3D(Xi[loja][1][\"Avr CPI\"],Xi[loja][1][\"Avr Unemp\"],sales_y_pred2,s=80)  \n",
    "    \n",
    "    # preparando as features para a validação cruzada\n",
    "    # precisamos separar as colunas da loja especificada, para a biblioteca trabalhar em cima da loja e feature correta\n",
    "    # Como Xteste e Xtreinamento já é a feature que eu preciso, basta concatenar a passar esse valar para a crossvalid\n",
    "    x = pd.concat([Xi[loja][0],Xi[loja][1]],ignore_index = True)\n",
    "    y = pd.concat([Yi[loja][0],Yi[loja][1]],ignore_index = True)\n",
    "    \n",
    "    # na validação cruzada o argumento cv é o número de partições que faremos\n",
    "    # como temos 52 dados, e 52 = 2²*13, vamos dividir em 13 grupos de 4 amostras\n",
    "    \n",
    "    # metricas para a validaçao cruzada\n",
    "    #'r2','neg_mean_squared_error','explained_variance'\n",
    "    # as metricas possuem um mnemonico correto, segue o link onde encontrar o mnemonico correto\n",
    "    # https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "    # para r²\n",
    "    r2 = cross_val_score(regr_1,x,y, scoring = 'r2', cv = 4)\n",
    "    r2_cross_m_m2.append(r2.mean())\n",
    "    r2_cross_std_m2.append(r2.std())\n",
    "    # erro médio quadratico\n",
    "    media = cross_val_score(regr,x,y, scoring = 'neg_mean_squared_error', cv = 4)\n",
    "    mean_square_cross_m_m2.append(media.mean())\n",
    "    mean_square_cross_std_m2.append(media.std())\n",
    "    # explanaid variance\n",
    "    var_exp = cross_val_score(regr,x,y, scoring = 'explained_variance', cv = 4)\n",
    "    explained_var_cross_m_m2.append(var_exp.mean())\n",
    "    explained_var_cross_std_m2.append(var_exp.std())\n",
    "    # error máximo absoluto\n",
    "    mean_max_error = cross_val_score(regr_1,x,y, scoring = 'neg_mean_absolute_error', cv = 4)\n",
    "    mean_max_error_m_m2.append(mean_max_error.mean())\n",
    "    mean_max_error_std_m2.append(mean_max_error.std())\n",
    "    #Importância\n",
    "    Importance_m2.append(regr_1.feature_importances_)\n",
    "\n",
    "    \n",
    "temp2 = 0\n",
    "fuel2 = 0\n",
    "cpi2 = 0\n",
    "unemp2 = 0\n",
    "for e in range(len(Importance_m2)):\n",
    "    temp2 += Importance_m2[e][0]\n",
    "    fuel2 += Importance_m2[e][1]\n",
    "    cpi2 += Importance_m2[e][2]\n",
    "    unemp2 += Importance_m2[e][3]\n",
    "temp2 = temp2/len(Importance_m2)\n",
    "fuel2 = fuel2/len(Importance_m2)\n",
    "cpi2 =  cpi2/len(Importance_m2)\n",
    "unemp2 = unemp2/len(Importance_m2)\n",
    "Importancia_Feature2 = [temp2,fuel2,cpi2,unemp2]\n",
    "print(Importancia_Feature2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==== Banco de Variaveis e Resultados ====\n",
    "#Informações unitarias (Cada Store ID)\n",
    "Coef_Reg3 = []\n",
    "Mean_Squared3 = []\n",
    "R_Squared3 = []\n",
    "explained_variance_m3 = []\n",
    "mean_max_abs_error_m3 = []\n",
    "#Predição\n",
    "Y_Prediction3 = []\n",
    "Importance_m3 = []\n",
    "#Criando Fig - Gráfico\n",
    "fig = plt.figure(figsize=(25,16))\n",
    "#Adição ao Eixo 2 Subplots com Projeção 3D\n",
    "ax3 = fig.add_subplot(121,projection=\"3d\")\n",
    "ax4 = fig.add_subplot(122,projection=\"3d\")\n",
    "#Definindo Labels\n",
    "ax3.set_xlabel(\"Avr Temperature\", fontsize=15)\n",
    "ax3.set_ylabel(\"Avr Fuel Price\", fontsize=15)\n",
    "ax3.set_zlabel(\"Weekly Sales\",fontsize=15)\n",
    "ax3.set_title(\"Avr Temp x Avr Fuel x Weekly Sales\",fontsize=20, fontweight=750)\n",
    "ax4.set_xlabel(\"Avr CPI\",fontsize=15)\n",
    "ax4.set_ylabel(\"Avr Unemployment\",fontsize=15)\n",
    "ax4.set_zlabel(\"Weekly Sales\",fontsize=15)\n",
    "ax4.set_title(\"Avr CPI x Avr Unemp x Weekly Sales\",fontsize=20, fontweight=750)\n",
    "#Definindo Angulatura de visualização\n",
    "ax3.view_init(35,35)\n",
    "ax4.view_init(35,35)\n",
    "\n",
    "# vetor para guardar as informações do cross validation e plotagem de gráficos futura\n",
    "# vetores com final m guardam a média dos valores\n",
    "r2_cross_m_m3 = []\n",
    "mean_square_cross_m_m3 = []\n",
    "explained_var_cross_m_m3 = []\n",
    "mean_max_error_m_m3 = []\n",
    "# já os vetores com std no final, guardam o desvio padrão da amostra\n",
    "r2_cross_std_m3 = []\n",
    "mean_square_cross_std_m3 = []\n",
    "explained_var_cross_std_m3 = []\n",
    "mean_max_error_std_m3 = []\n",
    "\n",
    "for loja in range(0,45):\n",
    "    #==== Criando Variaveis e Objetos ====\n",
    "    # Regrassão 2 - TreeRegressor\n",
    "    regr_2 = DecisionTreeRegressor(random_state=0)\n",
    "    regr_2.fit(Xi[loja][0], Yi[loja][0]) #Dados de Treinamento\n",
    "    # Previsão\n",
    "    sales_y_pred3 = regr_2.predict(Xi[loja][1])\n",
    "    Y_Prediction3.append(sales_y_pred3)\n",
    "    \n",
    "    #Coeficiente de regressão\n",
    "    const3 = [regr.intercept_]\n",
    "    const3.extend(regr.coef_)\n",
    "    Coef_Reg3.append(const)\n",
    "    \n",
    "    #Média Quadratica\n",
    "    m3 = mean_squared_error(Yi[loja][1], sales_y_pred3)\n",
    "    Mean_Squared3.append(m3)\n",
    "    #R Quadrado\n",
    "    r3 = r2_score(Yi[loja][1], sales_y_pred3)\n",
    "    R_Squared3.append(r3)\n",
    "    # explanaid variance \n",
    "    explained_variance_m3.append(explained_variance_score(Yi[loja][1],sales_y_pred3))\n",
    "    # Mean_absolute_error\n",
    "    mean_max_abs_error_m3.append(mean_absolute_error(Yi[loja][1],sales_y_pred3))\n",
    "    \n",
    "    #Plot Gráfico 3D\n",
    "    ax3.scatter3D(Xi[loja][1][\"Avr Temp\"],Xi[loja][1][\"Avr Fuel\"],sales_y_pred3,s=80) \n",
    "    ax4.scatter3D(Xi[loja][1][\"Avr CPI\"],Xi[loja][1][\"Avr Unemp\"],sales_y_pred3,s=80) \n",
    "    \n",
    "    # preparando as features para a validação cruzada\n",
    "    # precisamos separar as colunas da loja especificada, para a biblioteca trabalhar em cima da loja e feature correta\n",
    "    # Como Xteste e Xtreinamento já é a feature que eu preciso, basta concatenar a passar esse valar para a crossvalid\n",
    "    x = pd.concat([Xi[loja][0],Xi[loja][1]],ignore_index = True)\n",
    "    y = pd.concat([Yi[loja][0],Yi[loja][1]],ignore_index = True)\n",
    "    \n",
    "    # na validação cruzada o argumento cv é o número de partições que faremos\n",
    "    # como temos 52 dados, e 52 = 2²*13, vamos dividir em 13 grupos de 4 amostras\n",
    "    \n",
    "    # metricas para a validaçao cruzada\n",
    "    #'r2','neg_mean_squared_error','explained_variance'\n",
    "    # as metricas possuem um mnemonico correto, segue o link onde encontrar o mnemonico correto\n",
    "    # https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "    # para r²\n",
    "    r2 = cross_val_score(regr_1,x,y, scoring = 'r2', cv = 4)\n",
    "    r2_cross_m_m3.append(r2.mean())\n",
    "    r2_cross_std_m3.append(r2.std())\n",
    "    # erro médio quadratico\n",
    "    media = cross_val_score(regr,x,y, scoring = 'neg_mean_squared_error', cv = 4)\n",
    "    mean_square_cross_m_m3.append(media.mean())\n",
    "    mean_square_cross_std_m3.append(media.std())\n",
    "    # explanaid variance\n",
    "    var_exp = cross_val_score(regr,x,y, scoring = 'explained_variance', cv = 4)\n",
    "    explained_var_cross_m_m3.append(var_exp.mean())\n",
    "    explained_var_cross_std_m3.append(var_exp.std())\n",
    "    # error máximo absoluto\n",
    "    mean_max_error = cross_val_score(regr_2,x,y, scoring = 'neg_mean_absolute_error', cv = 4)\n",
    "    mean_max_error_m_m3.append(mean_max_error.mean())\n",
    "    mean_max_error_std_m3.append(mean_max_error.std())\n",
    "    #Importância\n",
    "    Importance_m3.append(regr_2.feature_importances_)\n",
    "\n",
    "    \n",
    "temp = 0\n",
    "fuel = 0\n",
    "cpi = 0\n",
    "unemp = 0\n",
    "for e in range(len(Importance_m3)):\n",
    "    temp += Importance_m3[e][0]\n",
    "    fuel += Importance_m3[e][1]\n",
    "    cpi += Importance_m3[e][2]\n",
    "    unemp += Importance_m3[e][3]\n",
    "temp = temp/len(Importance_m3)\n",
    "fuel = fuel/len(Importance_m3)\n",
    "cpi =  cpi/len(Importance_m3)\n",
    "unemp = unemp/len(Importance_m3)\n",
    "Importancia_Feature = [temp,fuel,cpi,unemp]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.barh([\"Temperature\",\"Fuel_Price\",\"CPI\",\"Unemployment\"],Importancia_Feature2)\n",
    "plt.title(\"Gráfico Barras - Importância Relativa das Features - Modelo 2\", fontsize=23, fontweight=600)\n",
    "plt.xlabel(\"Densidade de Importância\",fontsize=18)\n",
    "plt.ylabel(\"Features\",fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.barh([\"Temperature\",\"Fuel_Price\",\"CPI\",\"Unemployment\"],Importancia_Feature)\n",
    "plt.title(\"Gráfico Barras - Importância Relativa das Features - Modelo 3\", fontsize=23, fontweight=600)\n",
    "plt.xlabel(\"Densidade de Importância\",fontsize=18)\n",
    "plt.ylabel(\"Features\",fontsize=18)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
